{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd61157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder, OllamaDocumentEmbedder\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "from haystack.components.generators.chat import AzureOpenAIChatGenerator\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37d1db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curredev4omini\n",
      "https://currechattest.openai.azure.com/\n",
      "04307ef9d20746b48c2c7a29970314a4\n"
     ]
    }
   ],
   "source": [
    "# Read LAAMA_API_KEY and LAAMA_API_URL from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LAAMA_API_KEY = os.getenv(\"LAAMA_API_TOKEN\")\n",
    "LAAMA_API_URL = os.getenv(\"LAAMA_API_URL\")\n",
    "EMBEDDER_MODEL = \"snowflake-arctic-embed2\"\n",
    "CHAT_MODEL = \"deepseek-r1:7b\"\n",
    "\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT\")\n",
    "AZURE_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "print(AZURE_DEPLOYMENT)\n",
    "print(AZURE_ENDPOINT)\n",
    "print(os.getenv(\"AZURE_OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cec167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "ChatPromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "Could not read sources. Skipping it. Error: [Errno 2] No such file or directory: 'sources'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DocumentCleaner expects a List of Documents as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m rag_pipeline.connect(\u001b[33m\"\u001b[39m\u001b[33mprompt_builder\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# --- Print chunked documents after splitting ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m splitter_output = splitter.run({\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mcleaner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_file_converter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msources\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ohtu/osa0.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ohtu/osa1.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ohtu/osa2.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChunked documents:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m splitter_output[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/haystack/components/preprocessors/document_cleaner.py:105\u001b[39m, in \u001b[36mDocumentCleaner.run\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03mCleans up the documents.\u001b[39;00m\n\u001b[32m     96\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m:raises TypeError: if documents is not a list of Documents.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(documents, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m documents \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(documents[\u001b[32m0\u001b[39m], Document):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDocumentCleaner expects a List of Documents as input.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m cleaned_docs = []\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n",
      "\u001b[31mTypeError\u001b[39m: DocumentCleaner expects a List of Documents as input."
     ]
    }
   ],
   "source": [
    "llm = AzureOpenAIChatGenerator(\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    azure_deployment=AZURE_DEPLOYMENT,\n",
    ")\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "text_file_converter = TextFileToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OllamaDocumentEmbedder(\n",
    "    # api_key=LAAMA_API_KEY,\n",
    "    url=LAAMA_API_URL,\n",
    "    model=EMBEDDER_MODEL,\n",
    ")\n",
    "embedder._client._client.headers[\"token\"] = LAAMA_API_KEY\n",
    "\n",
    "writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing_pipeline.run(data={\"sources\": [\"../data/ohtu/osa0.md\", \"../data/ohtu/osa1.md\", \"../data/ohtu/osa2.md\" ]})\n",
    "\n",
    "text_embedder = OllamaTextEmbedder(\n",
    "    url=LAAMA_API_URL,\n",
    "    model=EMBEDDER_MODEL,\n",
    ")\n",
    "text_embedder._client._client.headers[\"token\"] = LAAMA_API_KEY\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "prompt_template = [\n",
    "    ChatMessage.from_user(\n",
    "      \"\"\"\n",
    "      Given these documents, answer the question.\n",
    "      Documents:\n",
    "      {% for doc in documents %}\n",
    "          {{ doc.content }}\n",
    "      {% endfor %}\n",
    "      Question: {{query}}\n",
    "      Answer:\n",
    "      \"\"\"\n",
    "    )\n",
    "]\n",
    "prompt_builder = ChatPromptBuilder(template=prompt_template)\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "# --- Print chunked documents after splitting ---\n",
    "splitter_output = splitter.run({\"documents\": cleaner.run({\"documents\": text_file_converter.run({\"sources\": [\"../data/ohtu/osa0.md\", \"../data/ohtu/osa1.md\", \"../data/ohtu/osa2.md\"]})[\"documents\"]})[\"documents\"]})\n",
    "print(\"Chunked documents:\")\n",
    "for doc in splitter_output[\"documents\"]:\n",
    "    print(doc.content)\n",
    "    print(\"---\")\n",
    "\n",
    "# --- Print prompt fed to LLM ---\n",
    "# Run up to prompt_builder\n",
    "query = \"Miten hyväksiluen miniprojektin?\"\n",
    "text_embedding = text_embedder.run({\"text\": query})[\"embedding\"]\n",
    "retrieved_docs = retriever.run({\"query_embedding\": text_embedding})[\"documents\"]\n",
    "prompt = prompt_builder.run({\"documents\": retrieved_docs, \"query\": query})[\"prompt\"]\n",
    "print(\"Prompt fed to LLM:\")\n",
    "print(prompt)\n",
    "\n",
    "# --- Run the full pipeline as before ---\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d46a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
